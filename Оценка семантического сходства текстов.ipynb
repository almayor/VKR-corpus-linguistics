{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babc0473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smile\\PycharmProjects\\VKR\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "# from textblob import TextBlob\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pymystem3 import Mystem  # lemmatizing from yandex\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "import en_core_web_sm\n",
    "import fr_core_news_sm\n",
    "\n",
    "import requests, matplotx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import razdel\n",
    "\n",
    "import string\n",
    "\n",
    "import math as m\n",
    "\n",
    "import re\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from scipy import spatial\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import razdel\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb19b5b",
   "metadata": {},
   "source": [
    "# Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cd5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_chars = string.punctuation + '\\n\\xa0«»\\t—'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e33a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Функция считывающая текст из pdf-файла в строку python\n",
    "    \n",
    "    Ввод: путь до файла на компьютере\n",
    "    Вывод: строка python\n",
    "    \"\"\"\n",
    "    resource_manager = PDFResourceManager()\n",
    "    fake_file_handle = io.StringIO()\n",
    "    converter = TextConverter(resource_manager, fake_file_handle)\n",
    "    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "        text = fake_file_handle.getvalue()\n",
    "\n",
    "    converter.close()\n",
    "    fake_file_handle.close()\n",
    "\n",
    "    if text:\n",
    "        return text\n",
    "    \n",
    "    \n",
    "def remove_chars_from_text(text, chars):\n",
    "    return \"\".join([ch for ch in text if ch not in chars])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_syllables(text, lang):\n",
    "    if lang == 'rus':\n",
    "        literas = 'аоиыуэ'\n",
    "    elif lang == 'spa':\n",
    "        literas = 'aеiоuáíóúé'\n",
    "    elif lang == 'eng':\n",
    "        literas = 'aeiou'\n",
    "    elif lang == 'fra':\n",
    "        literas = 'aeiouáíóúé'\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if i in literas:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def del_stopwords(lang, text_tokens):\n",
    "    if lang == 'rus':\n",
    "        lang_stopwords = stopwords.words(\"russian\")\n",
    "    elif lang == 'spa':\n",
    "        lang_stopwords = stopwords.words(\"spanish\")\n",
    "    elif lang == 'eng':\n",
    "        lang_stopwords = stopwords.words(\"english\")\n",
    "    elif lang == 'fra':\n",
    "        lang_stopwords = stopwords.words('french')\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in text_tokens:\n",
    "        if token not in lang_stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    #     print(filtered_tokens)\n",
    "    return (filtered_tokens)\n",
    "\n",
    "\n",
    "def lemmatize(lang, text):\n",
    "    if lang == 'rus':\n",
    "        m = Mystem()\n",
    "        lemmas = m.lemmatize(text)\n",
    "        lemmatize_str = \"\".join(lemmas).strip()\n",
    "    elif lang == 'spa':\n",
    "        nlp = es_core_news_sm.load()\n",
    "        list = []\n",
    "        for token in nlp(text):\n",
    "            list.append(token.lemma_)\n",
    "        lemmatize_str = ' '.join(list)\n",
    "    elif lang == 'eng':\n",
    "        nlp = en_core_web_sm.load()\n",
    "        list = []\n",
    "        for token in nlp(text):\n",
    "            list.append(token.lemma_)\n",
    "        lemmatize_str = ' '.join(list)\n",
    "    elif lang == 'fra':\n",
    "        nlp = fr_core_news_sm.load()\n",
    "        list = []\n",
    "        for token in nlp(text):\n",
    "            list.append(token.lemma_)\n",
    "        lemmatize_str = ' '.join(list)\n",
    "    return lemmatize_str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48aa24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(path, lang):\n",
    "    \n",
    "    text = extract_text_from_pdf(path) # Прочитали текст из PDF\n",
    "    sent_text = list(x.text for x in razdel.sentenize(text)) # Разделили строку на массив строк - предложений\n",
    "    \n",
    "    num_of_sent = len(sent_text)\n",
    "    print(\"Number of sentences: \",  num_of_sent)\n",
    "    \n",
    "    text = remove_chars_from_text(text, spec_chars) # Удалили из текста специальные символы\n",
    "    text_without_spaces = remove_chars_from_text(text, ' ') # Удалили из текста пробелы\n",
    "    num_of_symbols = len(text_without_spaces)\n",
    "    print(\"Number of symbols: \",  num_of_symbols)\n",
    "    \n",
    "    \n",
    "    text = text.lower() # Привели текст к нижнему регистру\n",
    "    \n",
    "    num_of_syllables = count_syllables(text_without_spaces, lang) # Количество слогов во всем тексте\n",
    "    print(\"Number of syllables: \",  num_of_syllables)\n",
    "    \n",
    "    text_tokens = word_tokenize(text) # Разделили строку на массив строк - слов\n",
    "    count_tokens_with_stopwords = len(text_tokens) # Количество слов во всем тексте\n",
    "    print(\"Number of tokens with stopword: \",  count_tokens_with_stopwords)\n",
    "    \n",
    "    count_words_w_3_syllables = 0\n",
    "    for word in text_tokens:\n",
    "        w_cnt_syl_3 = count_syllables(str(word), lang)\n",
    "        if w_cnt_syl_3 >= 3:\n",
    "            count_words_w_3_syllables += 1\n",
    "            \n",
    "    print(\"Number of tokens with 3 syllables: \",  count_words_w_3_syllables) # Количество слов с 3 слогами или больше\n",
    "    \n",
    "    filtered_tokens = del_stopwords(lang, text_tokens) # Удалили из массива слов стоп-слова\n",
    "    count_tokens = len(filtered_tokens)\n",
    "    print(\"Number of tokens without stopword: \",  count_tokens)\n",
    "    \n",
    "    filtered_string = ' '.join(filtered_tokens) # Преобразовали список в строку\n",
    "    \n",
    "    lemmatize_str = lemmatize(lang, filtered_string) # Привели слова к леммам\n",
    "    lemmatize_tokens = lemmatize_str.split() \n",
    "    \n",
    "    unique_tokens = list(set(lemmatize_tokens))\n",
    "    unique_tokens_with_stopwords = list(set(text_tokens))\n",
    "    count_unique_tokens_with_stopwords = len(unique_tokens_with_stopwords)\n",
    "    count_unique_tokens = len(unique_tokens)\n",
    "    print(\"Number of unique tokens without stopword: \",  count_unique_tokens)\n",
    "    print(\"Number of unique tokens with stopword: \",  count_unique_tokens_with_stopwords)\n",
    "    \n",
    "    res = {\n",
    "        'num_of_sent' : num_of_sent,\n",
    "        'num_of_symbols' : num_of_symbols,\n",
    "        'num_of_syllables' : num_of_syllables,\n",
    "        'count_tokens_with_stopwords' : count_tokens_with_stopwords,\n",
    "        'count_words_w_3_syllables' : count_words_w_3_syllables,\n",
    "        'count_tokens' : count_tokens,\n",
    "        'count_unique_tokens' : count_unique_tokens,\n",
    "        'count_unique_tokens_with_stopwords' : count_unique_tokens_with_stopwords,\n",
    "        'lemmatize_tokens' : lemmatize_tokens,\n",
    "        'unique_tokens' : unique_tokens,\n",
    "        'text_tokens' : text_tokens\n",
    "    }\n",
    "\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2000124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  27\n",
      "Number of symbols:  2477\n",
      "Number of syllables:  746\n",
      "Number of tokens with stopword:  459\n",
      "Number of tokens with 3 syllables:  103\n",
      "Number of tokens without stopword:  307\n",
      "Number of unique tokens without stopword:  237\n",
      "Number of unique tokens with stopword:  326\n"
     ]
    }
   ],
   "source": [
    "link = \"C:/Users/smile/PycharmProjects/VKR/necessary_docs/Непрерывность_парков/5_Непрерывность_парков.pdf\" \n",
    "lang = \"rus\"\n",
    "year = 1959\n",
    "\n",
    "res = text_preprocessing(link, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cd176",
   "metadata": {},
   "source": [
    "# Подсчет частотности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1e427e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_tokens_dict = {}\n",
    "for word in res['lemmatize_tokens']:\n",
    "        if word in lemmatize_tokens_dict:\n",
    "            lemmatize_tokens_dict[word] = lemmatize_tokens_dict[word] + 1\n",
    "        else:\n",
    "            lemmatize_tokens_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "088b632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=50):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f61f7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dict_freq(unique_tokens, lang, year):\n",
    "    inf_unique_tokens = [x + '_INF' for x in unique_tokens]\n",
    "    list_batches = batch(inf_unique_tokens)\n",
    "    num_batches = (len(list(list_batches)))\n",
    "    list_batches = batch(inf_unique_tokens)\n",
    "    cnt = 0\n",
    "    dict_freq = {}\n",
    "    for item in list_batches:\n",
    "        cnt += 1\n",
    "        print('Batch', cnt, 'of',  num_batches)\n",
    "        inf_unique_tokens_str = ','.join(item)\n",
    "\n",
    "        if lang == 'rus':\n",
    "            params = {\n",
    "                \"content\": inf_unique_tokens_str,\n",
    "                \"year_start\": str(year - 1),\n",
    "                \"year_end\": str(year),\n",
    "                \"corpus\": \"ru-2019\"\n",
    "            }\n",
    "        elif lang == 'spa':\n",
    "            params = {\n",
    "                \"content\": inf_unique_tokens_str,\n",
    "                \"year_start\": str(year - 1),\n",
    "                \"year_end\": str(year),\n",
    "                \"corpus\": \"es-2019\"\n",
    "            }\n",
    "        elif lang == 'eng':\n",
    "            params = {\n",
    "                \"content\": inf_unique_tokens_str,\n",
    "                \"year_start\": str(year - 1),\n",
    "                \"year_end\": str(year),\n",
    "                \"corpus\": \"en-2019\"\n",
    "            }\n",
    "        elif lang == 'fra':\n",
    "            params = {\n",
    "                \"content\": inf_unique_tokens_str,\n",
    "                \"year_start\": str(year - 1),\n",
    "                \"year_end\": str(year),\n",
    "                \"corpus\": \"fr-2019\"\n",
    "            }\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.5359.125 Safari/537.36\",\n",
    "        }\n",
    "\n",
    "        r = requests.get(\"https://books.google.com/ngrams/json\", params=params, headers=headers, timeout=1000)\n",
    "        html = r.text\n",
    "        time_series = pd.read_json(html, typ=\"series\")\n",
    "\n",
    "        for i in time_series:\n",
    "            if i['type'] == 'EXPANSION' and len(i['timeseries']) == 2:\n",
    "                if i['parent'][:int(i['parent'].find(\"_\"))] not in dict_freq:\n",
    "                    dict_freq[i['parent'][:int(i['parent'].find(\"_\"))]] = [i['timeseries'][1]*100, 1]\n",
    "                else:\n",
    "                    dict_freq[i['parent'][:int(i['parent'].find(\"_\"))]][0] = dict_freq[i['parent'][:int(i['parent'].find(\"_\"))]][0] \\\n",
    "                                                                            + i['timeseries'][1]*100\n",
    "                    dict_freq[i['parent'][:int(i['parent'].find(\"_\"))]][1] = dict_freq[i['parent'][:int(i['parent'].find(\"_\"))]][1] + 1\n",
    "\n",
    "\n",
    "    return dict_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b741c5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 of 5\n",
      "Batch 2 of 5\n",
      "Batch 3 of 5\n",
      "Batch 4 of 5\n",
      "Batch 5 of 5\n"
     ]
    }
   ],
   "source": [
    "dict_freq = do_dict_freq(res['unique_tokens'], lang, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1709c4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>женщина</td>\n",
       "      <td>0.014834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>северный</td>\n",
       "      <td>0.012934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>кресло</td>\n",
       "      <td>0.009383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>весь</td>\n",
       "      <td>0.007758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>другой</td>\n",
       "      <td>0.007524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>входить</td>\n",
       "      <td>0.007230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>самый</td>\n",
       "      <td>0.006995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>щека</td>\n",
       "      <td>0.006790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>слово</td>\n",
       "      <td>0.006776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>человек</td>\n",
       "      <td>0.006611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frequency\n",
       "0   женщина   0.014834\n",
       "1  северный   0.012934\n",
       "2    кресло   0.009383\n",
       "3      весь   0.007758\n",
       "4    другой   0.007524\n",
       "5   входить   0.007230\n",
       "6     самый   0.006995\n",
       "7      щека   0.006790\n",
       "8     слово   0.006776\n",
       "9   человек   0.006611"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>строка</td>\n",
       "      <td>6.378881e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>мир</td>\n",
       "      <td>1.080981e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>кто</td>\n",
       "      <td>1.107733e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>голова</td>\n",
       "      <td>1.439822e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>мужчина</td>\n",
       "      <td>1.454684e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>дверь</td>\n",
       "      <td>2.981213e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>дело</td>\n",
       "      <td>2.989573e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>жажда</td>\n",
       "      <td>3.071346e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>смысл</td>\n",
       "      <td>3.728413e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>лестница</td>\n",
       "      <td>3.955860e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word     Frequency\n",
       "0    строка  6.378881e-08\n",
       "1       мир  1.080981e-07\n",
       "2       кто  1.107733e-07\n",
       "3    голова  1.439822e-07\n",
       "4   мужчина  1.454684e-07\n",
       "5     дверь  2.981213e-07\n",
       "6      дело  2.989573e-07\n",
       "7     жажда  3.071346e-07\n",
       "8     смысл  3.728413e-07\n",
       "9  лестница  3.955860e-07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_calculated_freq = {}\n",
    "for i in dict_freq:\n",
    "    words_calculated_freq[i] = dict_freq[i][0]/dict_freq[i][1]\n",
    "    \n",
    "\n",
    "def most_least_common(words_calculated_freq, n=10):\n",
    "    sorted_freq = dict(sorted(words_calculated_freq.items(), key=lambda item: item[1]))\n",
    "    from collections import Counter\n",
    "    d = Counter(words_calculated_freq)\n",
    "\n",
    "\n",
    "    sum1 = 0\n",
    "    for k, v in d.most_common(n):\n",
    "        sum1 += v\n",
    "        \n",
    "    most_common = d.most_common(n)\n",
    "    least_common = d.most_common()[:-n-1:-1]\n",
    "    \n",
    "    return most_common, least_common\n",
    "\n",
    "most_common, least_common = most_least_common(words_calculated_freq)\n",
    "\n",
    "df_most_common = pd.DataFrame(most_common, columns =['Word', 'Frequency'])\n",
    "display(df_most_common)\n",
    "\n",
    "df_least_common = pd.DataFrame(least_common, columns =['Word', 'Frequency'])\n",
    "display(df_least_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a9ae8265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average frequency of use in the corpus: 0.0012439060 %\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "n = 0\n",
    "for i in words_calculated_freq:\n",
    "    sum = sum + lemmatize_tokens_dict[i] * words_calculated_freq[i]\n",
    "    n = n + lemmatize_tokens_dict[i]\n",
    "\n",
    "avg_freq = sum / n\n",
    "print('The average frequency of use in the corpus: {:.10f} %'.format(avg_freq)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5dc59",
   "metadata": {},
   "source": [
    "# Расчет статистических характеристик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89e738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_sent = res['num_of_sent']\n",
    "num_of_symbols = res['num_of_symbols']\n",
    "num_of_syllables = res['num_of_syllables']\n",
    "count_tokens_with_stopwords = res['count_tokens_with_stopwords']\n",
    "count_words_w_3_syllables = res['count_words_w_3_syllables']\n",
    "count_tokens = res['count_tokens']\n",
    "count_unique_tokens = res['count_unique_tokens']\n",
    "count_unique_tokens_with_stopwords = res['count_unique_tokens_with_stopwords']\n",
    "lemmatize_tokens = res['lemmatize_tokens']\n",
    "unique_tokens = res['unique_tokens']\n",
    "text_tokens = res['text_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e6075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 17.0\n",
      "Average word length: 5.4\n",
      "Frequency of use of stop words: 0.33\n",
      "Lexical diversity (Type-Token Ratio): 0.71\n",
      "Lexical diversity (Guiraud's Root TTR): 15.22\n",
      "Lexical diversity (Dugast's Uber Index): 109.79\n"
     ]
    }
   ],
   "source": [
    "Lp = count_tokens_with_stopwords / num_of_sent\n",
    "print(\"Average sentence length:\", round(Lp, 2))\n",
    "Lpp = num_of_symbols / count_tokens_with_stopwords\n",
    "print(\"Average word length:\", round(Lpp, 2))\n",
    "Pp = (count_tokens_with_stopwords - count_tokens) / count_tokens_with_stopwords\n",
    "print(\"Frequency of use of stop words:\", round(Pp, 2))\n",
    "TTR = count_unique_tokens_with_stopwords / count_tokens_with_stopwords\n",
    "print(\"Lexical diversity (Type-Token Ratio):\", round(TTR, 2))\n",
    "R =  count_unique_tokens_with_stopwords / m.sqrt(count_tokens_with_stopwords)\n",
    "print(\"Lexical diversity (Guiraud's Root TTR):\", round(R, 2))\n",
    "U = (m.log(count_tokens_with_stopwords))**2 / (m.log(count_tokens_with_stopwords) - m.log(count_unique_tokens_with_stopwords))\n",
    "print(\"Lexical diversity (Dugast's Uber Index):\", round(U, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11cd89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical diversity (Yule's K): 59.05\n"
     ]
    }
   ],
   "source": [
    "result = {}    \n",
    "for word in text_tokens:\n",
    "    result[word] = result[word] + 1 if word in result else 1\n",
    "\n",
    "df_res = pd.DataFrame(result.items(), columns=['word', 'count'])\n",
    "a_df = df_res.groupby(['count']).count()\n",
    "a_df = a_df.reset_index()\n",
    "# display(a_df)\n",
    "# print(a_df.shape)\n",
    "k_sum = 0\n",
    "for i in range(a_df.shape[0]):\n",
    "#     print(a_df.loc[i, 'count'])\n",
    "    k_sum = k_sum + int(a_df.loc[i, 'word']) * ((int(a_df.loc[i, 'count'])/count_tokens_with_stopwords))**2\n",
    "    \n",
    "K = 10**4 *(-1/count_tokens_with_stopwords+k_sum)\n",
    "print(\"Lexical diversity (Yule's K):\", round(K, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2058dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Index (Flesch–Kincaid): 10.22\n",
      "Readability Index (Gunning Fog): 15.78\n"
     ]
    }
   ],
   "source": [
    "FK = 0.39 * (count_tokens_with_stopwords/num_of_sent) + 11.8 * (num_of_syllables / count_tokens_with_stopwords) - 15.59\n",
    "print(\"Readability Index (Flesch–Kincaid):\", round(FK, 2))\n",
    "G = 0.4 * ((count_tokens_with_stopwords/num_of_sent) + 100 * ( count_words_w_3_syllables / count_tokens_with_stopwords))\n",
    "print(\"Readability Index (Gunning Fog):\", round(G, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75081b8",
   "metadata": {},
   "source": [
    "# Растчет коэффициента семантической схожести текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6dd4c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_st = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "56e4a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            for page in PDFPage.get_pages(fh,\n",
    "                                          caching=True,\n",
    "                                          check_extractable=True):\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "            text = fake_file_handle.getvalue()\n",
    "\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        if text:\n",
    "            return text\n",
    "        \n",
    "def clean_text(text):\n",
    "        new_text = re.sub('\\n', ' ', text)\n",
    "        new_text = re.sub(\" +\", \" \", new_text)\n",
    "        sent = list(x.text for x in razdel.sentenize(new_text))\n",
    "        return sent\n",
    "    \n",
    "    \n",
    "def get_batch(iter1, iter2, batch_size):\n",
    "        l1 = len(iter1)\n",
    "        l2 = len(iter2)\n",
    "        k = int(round(batch_size * l2/l1))    \n",
    "        kdx = 0 - k\n",
    "        for ndx in range(0, l1, batch_size):\n",
    "            kdx += k\n",
    "            yield iter1[ndx:min(ndx + batch_size, l1)], iter2[kdx:min(kdx + k, l2)]\n",
    "            \n",
    "            \n",
    "def get_sim_matrix(vec1, vec2, window=10):\n",
    "    sim_matrix=np.zeros((len(vec1), len(vec2)))\n",
    "    k = len(vec1)/len(vec2)\n",
    "    for i in range(len(vec1)):\n",
    "        for j in range(len(vec2)):\n",
    "            if (j*k > i-window) & (j*k < i+window):\n",
    "              sim = 1 - spatial.distance.cosine(vec1[i], vec2[j])\n",
    "              sim_matrix[i,j] = sim\n",
    "    return sim_matrix\n",
    "    \n",
    "def get_pairs(ru_lines, de_lines, sim_matrix, threshold):\n",
    "        ru = []\n",
    "        de = []\n",
    "        sims = []\n",
    "        for i in range(sim_matrix.shape[0]):\n",
    "            for j in range(sim_matrix.shape[1]):\n",
    "                if sim_matrix[i,j] >= threshold:\n",
    "                    ru.append(ru_lines[i])\n",
    "                    de.append(de_lines[j])\n",
    "                    sims.append(sim_matrix[i,j])                \n",
    "        return ru, de, sims\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eb1f6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sim(path1, path2):\n",
    "    text1 = extract_text_from_pdf(path1)\n",
    "    text2 = extract_text_from_pdf(path2)\n",
    "    \n",
    "    sent1 = clean_text(text1)\n",
    "    sent2 = clean_text(text2)\n",
    "    \n",
    "    \n",
    "    batch_number = 0\n",
    "    total_pairs = 0\n",
    "    batch_size = 50\n",
    "    window = 10\n",
    "    threshold = 0.3\n",
    "    \n",
    "    vectors1, vectors2 = [], []\n",
    "    \n",
    "    for lines1_batch, lines2_batch in get_batch(sent1, sent2, batch_size):\n",
    "        batch_number += 1\n",
    "        vectors1 = [*vectors1, *model_st.encode(lines1_batch)]\n",
    "        vectors2 = [*vectors2, *model_st.encode(lines2_batch)]\n",
    "        \n",
    "    sim_matrix = get_sim_matrix(vectors1, vectors2, window)    \n",
    "    sim_matrix_best = np.zeros_like(sim_matrix)\n",
    "    sim_matrix_best[range(len(sim_matrix)), sim_matrix.argmax(1)] = sim_matrix[range(len(sim_matrix)), sim_matrix.argmax(1)]\n",
    "    \n",
    "\n",
    "    res1, res2, sims = get_pairs(sent1, sent2, sim_matrix_best, threshold)\n",
    "    sim_sum = 0\n",
    "    counter = 0\n",
    "    for x, y, s in zip(res1, res2, sims):\n",
    "        counter += 1\n",
    "        sim_sum += s\n",
    "        \n",
    "    similarity = round(sim_sum/counter, 2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "32382af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"C:/Users/smile/PycharmProjects/VKR/necessary_docs/Непрерывность_парков/1_Continuidad_de_los_parques.pdf\",\n",
    "    \"C:/Users/smile/PycharmProjects/VKR/necessary_docs/Непрерывность_парков/2_Continuité-des-Parcs.pdf\",\n",
    "    \"C:/Users/smile/PycharmProjects/VKR/necessary_docs/Непрерывность_парков/3_the-continuity-of-parks.pdf\",\n",
    "    \"C:/Users/smile/PycharmProjects/VKR/necessary_docs/Непрерывность_парков/4_Непрерывность_парков.pdf\",\n",
    "    \"C:/Users/smile/PycharmProjects/VKR/necessary_docs/Непрерывность_парков/5_Непрерывность_парков.pdf\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6756de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_matrix = []\n",
    "for p1 in paths:\n",
    "    res_list = []\n",
    "    for p2 in paths:\n",
    "        res_list.append(calculate_sim(p1, p2))\n",
    "    res_matrix.append(res_list) \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "71eb9c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.79, 0.85, 0.78, 0.71],\n",
       " [0.71, 1.0, 0.69, 0.66, 0.59],\n",
       " [0.85, 0.76, 1.0, 0.77, 0.69],\n",
       " [0.78, 0.72, 0.77, 1.0, 0.77],\n",
       " [0.7, 0.65, 0.69, 0.78, 1.0]]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c8c17d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(res_matrix)):\n",
    "    for j in range(len(res_matrix)):\n",
    "        res_matrix[i][j] = res_matrix[j][i] = max(res_matrix[i][j], res_matrix[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a374faf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.79, 0.85, 0.78, 0.71],\n",
       " [0.79, 1.0, 0.76, 0.72, 0.65],\n",
       " [0.85, 0.76, 1.0, 0.77, 0.69],\n",
       " [0.78, 0.72, 0.77, 1.0, 0.78],\n",
       " [0.71, 0.65, 0.69, 0.78, 1.0]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86209f2",
   "metadata": {},
   "source": [
    "# Корреляционный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "774c32e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lp</th>\n",
       "      <th>Lpp</th>\n",
       "      <th>Pp</th>\n",
       "      <th>TTR</th>\n",
       "      <th>R</th>\n",
       "      <th>U</th>\n",
       "      <th>K</th>\n",
       "      <th>FK</th>\n",
       "      <th>G</th>\n",
       "      <th>F</th>\n",
       "      <th>SIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-2</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.57</td>\n",
       "      <td>11.79</td>\n",
       "      <td>6.89</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.00291</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3</th>\n",
       "      <td>3.73</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.20</td>\n",
       "      <td>55.59</td>\n",
       "      <td>6.66</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00508</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-4</th>\n",
       "      <td>2.39</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.95</td>\n",
       "      <td>69.20</td>\n",
       "      <td>64.94</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.00086</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-5</th>\n",
       "      <td>2.46</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.10</td>\n",
       "      <td>49.21</td>\n",
       "      <td>60.47</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.00059</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3</th>\n",
       "      <td>5.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3.37</td>\n",
       "      <td>67.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.00217</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.82</td>\n",
       "      <td>72.77</td>\n",
       "      <td>53.15</td>\n",
       "      <td>2.34</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-5</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.97</td>\n",
       "      <td>52.78</td>\n",
       "      <td>60.47</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.00232</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-4</th>\n",
       "      <td>6.12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.48</td>\n",
       "      <td>69.40</td>\n",
       "      <td>120.53</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00422</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-5</th>\n",
       "      <td>6.19</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.63</td>\n",
       "      <td>49.41</td>\n",
       "      <td>116.03</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-5</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.85</td>\n",
       "      <td>19.99</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lp   Lpp    Pp   TTR     R      U       K    FK     G        F   SIM\n",
       "1-2  1.70  0.35  0.04  0.04  0.13   3.57   11.79  6.89  6.77  0.00291  0.79\n",
       "1-3  3.73  0.32  0.02  0.02  0.47   0.20   55.59  6.66  4.06  0.00508  0.85\n",
       "1-4  2.39  0.63  0.17  0.23  3.95  69.20   64.94  4.55  3.22  0.00086  0.78\n",
       "1-5  2.46  0.63  0.16  0.27  3.10  49.21   60.47  3.86  3.15  0.00059  0.71\n",
       "2-3  5.43  0.03  0.06  0.02  0.34   3.37   67.38  0.23  2.71  0.00217  0.76\n",
       "2-4  0.67  0.98  0.13  0.27  3.82  72.77   53.15  2.34  3.55  0.00214  0.72\n",
       "2-5  0.76  0.98  0.12  0.23  2.97  52.78   60.47  3.03  3.62  0.00232  0.65\n",
       "3-4  6.12  0.95  0.19  0.25  3.48  69.40  120.53  2.11  0.84  0.00422  0.77\n",
       "3-5  6.19  0.95  0.18  0.21  2.63  49.41  116.03  2.80  0.91  0.00449  0.69\n",
       "4-5  0.07  0.00  0.01  0.04  0.85  19.99    4.47  0.69  0.07  0.00027  0.78"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Lp': [1.7, 3.73, 2.39, 2.46, 5.43, 0.67, 0.76, 6.12, 6.19, 0.07],\n",
    "       'Lpp': [0.35, 0.32, 0.63, 0.63, 0.03, 0.98, 0.98, 0.95, 0.95, 0],\n",
    "       'Pp': [0.04, 0.02, 0.17, 0.16, 0.06, 0.13, 0.12, 0.19, 0.18, 0.01],\n",
    "       'TTR': [0.04, 0.02, 0.23, 0.27, 0.02, 0.27, 0.23, 0.25, 0.21, 0.04],\n",
    "       'R': [0.13, 0.47, 3.95, 3.1, 0.34, 3.82, 2.97, 3.48, 2.63, 0.85],\n",
    "       'U': [3.57, 0.2, 69.2, 49.21, 3.37, 72.77, 52.78, 69.4, 49.41, 19.99],\n",
    "       'K': [11.79, 55.59, 64.94, 60.47, 67.38, 53.15, 60.47, 120.53, 116.03, 4.47],\n",
    "       'FK': [6.89, 6.66, 4.55, 3.86, 0.23, 2.34, 3.03, 2.11, 2.8, 0.69],\n",
    "       'G': [6.77, 4.06, 3.22, 3.15, 2.71, 3.55, 3.62, 0.84, 0.91, 0.07],\n",
    "       'F': [0.00291, 0.00508, 0.00086, 0.00059, 0.00217, 0.00214, 0.00232, 0.00422, 0.00449, 0.00027],\n",
    "       'SIM': [0.79, 0.85, 0.78, 0.71, 0.76, 0.72, 0.65, 0.77, 0.69, 0.78]}\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['Lp', 'Lpp', 'Pp', 'TTR', 'R', 'U', 'K', 'FK', 'G', 'F', 'SIM'], \n",
    "                 index = ['1-2', '1-3', '1-4', '1-5', '2-3', '2-4', '2-5', '3-4', '3-5', '4-5'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7cfd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_idx = ['Lp', 'Lpp', 'Pp', 'TTR', 'R', 'U', 'K', 'FK', 'G', 'F', 'SIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "65cf1ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lp', 'Lpp', 'Pp', 'TTR', 'R', 'U', 'K', 'FK', 'G', 'F', 'SIM'], dtype='object')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d08520cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lp</th>\n",
       "      <th>Lpp</th>\n",
       "      <th>Pp</th>\n",
       "      <th>TTR</th>\n",
       "      <th>R</th>\n",
       "      <th>U</th>\n",
       "      <th>K</th>\n",
       "      <th>FK</th>\n",
       "      <th>G</th>\n",
       "      <th>F</th>\n",
       "      <th>SIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lp</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.114570</td>\n",
       "      <td>0.372758</td>\n",
       "      <td>-0.004600</td>\n",
       "      <td>-0.026132</td>\n",
       "      <td>-0.030518</td>\n",
       "      <td>0.808243</td>\n",
       "      <td>-0.119856</td>\n",
       "      <td>-0.314134</td>\n",
       "      <td>0.635022</td>\n",
       "      <td>0.104984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lpp</th>\n",
       "      <td>0.114570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816080</td>\n",
       "      <td>0.878382</td>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.827391</td>\n",
       "      <td>0.627353</td>\n",
       "      <td>0.060906</td>\n",
       "      <td>-0.054355</td>\n",
       "      <td>0.287979</td>\n",
       "      <td>-0.621243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pp</th>\n",
       "      <td>0.372758</td>\n",
       "      <td>0.816080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905508</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>0.868975</td>\n",
       "      <td>0.775619</td>\n",
       "      <td>-0.134650</td>\n",
       "      <td>-0.277395</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>-0.546212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTR</th>\n",
       "      <td>-0.004600</td>\n",
       "      <td>0.878382</td>\n",
       "      <td>0.905508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966775</td>\n",
       "      <td>0.951001</td>\n",
       "      <td>0.536449</td>\n",
       "      <td>-0.117982</td>\n",
       "      <td>-0.192045</td>\n",
       "      <td>-0.129891</td>\n",
       "      <td>-0.647046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>-0.026132</td>\n",
       "      <td>0.824821</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>0.966775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987074</td>\n",
       "      <td>0.522675</td>\n",
       "      <td>-0.155043</td>\n",
       "      <td>-0.258362</td>\n",
       "      <td>-0.168986</td>\n",
       "      <td>-0.528606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>-0.030518</td>\n",
       "      <td>0.827391</td>\n",
       "      <td>0.868975</td>\n",
       "      <td>0.951001</td>\n",
       "      <td>0.987074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.513216</td>\n",
       "      <td>-0.224386</td>\n",
       "      <td>-0.314349</td>\n",
       "      <td>-0.160372</td>\n",
       "      <td>-0.532731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.808243</td>\n",
       "      <td>0.627353</td>\n",
       "      <td>0.775619</td>\n",
       "      <td>0.536449</td>\n",
       "      <td>0.522675</td>\n",
       "      <td>0.513216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.205493</td>\n",
       "      <td>-0.425754</td>\n",
       "      <td>0.535288</td>\n",
       "      <td>-0.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FK</th>\n",
       "      <td>-0.119856</td>\n",
       "      <td>0.060906</td>\n",
       "      <td>-0.134650</td>\n",
       "      <td>-0.117982</td>\n",
       "      <td>-0.155043</td>\n",
       "      <td>-0.224386</td>\n",
       "      <td>-0.205493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743585</td>\n",
       "      <td>0.332144</td>\n",
       "      <td>0.385538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>-0.314134</td>\n",
       "      <td>-0.054355</td>\n",
       "      <td>-0.277395</td>\n",
       "      <td>-0.192045</td>\n",
       "      <td>-0.258362</td>\n",
       "      <td>-0.314349</td>\n",
       "      <td>-0.425754</td>\n",
       "      <td>0.743585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071376</td>\n",
       "      <td>0.169431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.635022</td>\n",
       "      <td>0.287979</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>-0.129891</td>\n",
       "      <td>-0.168986</td>\n",
       "      <td>-0.160372</td>\n",
       "      <td>0.535288</td>\n",
       "      <td>0.332144</td>\n",
       "      <td>0.071376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.201004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIM</th>\n",
       "      <td>0.104984</td>\n",
       "      <td>-0.621243</td>\n",
       "      <td>-0.546212</td>\n",
       "      <td>-0.647046</td>\n",
       "      <td>-0.528606</td>\n",
       "      <td>-0.532731</td>\n",
       "      <td>-0.302800</td>\n",
       "      <td>0.385538</td>\n",
       "      <td>0.169431</td>\n",
       "      <td>0.201004</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Lp       Lpp        Pp       TTR         R         U         K  \\\n",
       "Lp   1.000000  0.114570  0.372758 -0.004600 -0.026132 -0.030518  0.808243   \n",
       "Lpp  0.114570  1.000000  0.816080  0.878382  0.824821  0.827391  0.627353   \n",
       "Pp   0.372758  0.816080  1.000000  0.905508  0.881746  0.868975  0.775619   \n",
       "TTR -0.004600  0.878382  0.905508  1.000000  0.966775  0.951001  0.536449   \n",
       "R   -0.026132  0.824821  0.881746  0.966775  1.000000  0.987074  0.522675   \n",
       "U   -0.030518  0.827391  0.868975  0.951001  0.987074  1.000000  0.513216   \n",
       "K    0.808243  0.627353  0.775619  0.536449  0.522675  0.513216  1.000000   \n",
       "FK  -0.119856  0.060906 -0.134650 -0.117982 -0.155043 -0.224386 -0.205493   \n",
       "G   -0.314134 -0.054355 -0.277395 -0.192045 -0.258362 -0.314349 -0.425754   \n",
       "F    0.635022  0.287979  0.050292 -0.129891 -0.168986 -0.160372  0.535288   \n",
       "SIM  0.104984 -0.621243 -0.546212 -0.647046 -0.528606 -0.532731 -0.302800   \n",
       "\n",
       "           FK         G         F       SIM  \n",
       "Lp  -0.119856 -0.314134  0.635022  0.104984  \n",
       "Lpp  0.060906 -0.054355  0.287979 -0.621243  \n",
       "Pp  -0.134650 -0.277395  0.050292 -0.546212  \n",
       "TTR -0.117982 -0.192045 -0.129891 -0.647046  \n",
       "R   -0.155043 -0.258362 -0.168986 -0.528606  \n",
       "U   -0.224386 -0.314349 -0.160372 -0.532731  \n",
       "K   -0.205493 -0.425754  0.535288 -0.302800  \n",
       "FK   1.000000  0.743585  0.332144  0.385538  \n",
       "G    0.743585  1.000000  0.071376  0.169431  \n",
       "F    0.332144  0.071376  1.000000  0.201004  \n",
       "SIM  0.385538  0.169431  0.201004  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_df = pd.DataFrame()\n",
    "for i in df.columns:\n",
    "    list_corr = []\n",
    "    for j in df.columns:\n",
    "        list_corr.append(df[i].corr(df[j]))\n",
    "#     print(i)\n",
    "#     display(pd.DataFrame(list_corr, index = list_idx ))\n",
    "    corr_df[i] = pd.DataFrame(list_corr, index = list_idx )\n",
    "    \n",
    "display(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7bb319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.4.0\n",
      "altgraph==0.17.3\n",
      "anyio==3.6.2\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.2.3\n",
      "asttokens==2.2.1\n",
      "astunparse==1.6.3\n",
      "attrs==22.2.0\n",
      "auto-py-to-exe==2.27.0\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.11.1\n",
      "bert-for-tf2==0.14.9\n",
      "bleach==5.0.1\n",
      "blis==0.7.9\n",
      "bottle==0.12.23\n",
      "bottle-websocket==0.2.9\n",
      "cachetools==5.3.0\n",
      "catalogue==2.0.8\n",
      "certifi==2022.12.7\n",
      "cffi==1.15.1\n",
      "charset-normalizer==2.1.1\n",
      "click==8.1.3\n",
      "colorama==0.4.6\n",
      "comm==0.1.2\n",
      "confection==0.0.4\n",
      "contourpy==1.0.6\n",
      "cycler==0.11.0\n",
      "cymem==2.0.7\n",
      "debugpy==1.6.5\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "Eel==0.14.0\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl\n",
      "entrypoints==0.4\n",
      "es-core-news-sm @ https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl\n",
      "et-xmlfile==1.1.0\n",
      "executing==1.2.0\n",
      "fastjsonschema==2.16.2\n",
      "filelock==3.11.0\n",
      "flatbuffers==23.3.3\n",
      "fonttools==4.38.0\n",
      "fqdn==1.5.1\n",
      "fr-core-news-sm @ https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl\n",
      "future==0.18.3\n",
      "gast==0.4.0\n",
      "gevent==22.10.2\n",
      "gevent-websocket==0.10.1\n",
      "google-auth==2.17.3\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "greenlet==2.0.1\n",
      "grpcio==1.53.0\n",
      "h5py==3.8.0\n",
      "huggingface-hub==0.13.4\n",
      "idna==3.4\n",
      "importlib-metadata==6.0.0\n",
      "ipykernel==6.20.2\n",
      "ipython==8.8.0\n",
      "ipython-genutils==0.2.0\n",
      "isoduration==20.11.0\n",
      "jax==0.4.8\n",
      "jedi==0.18.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "jsonpointer==2.3\n",
      "jsonschema==4.17.3\n",
      "jupyter-events==0.6.3\n",
      "jupyter_client==7.4.9\n",
      "jupyter_core==5.1.3\n",
      "jupyter_server==2.1.0\n",
      "jupyter_server_terminals==0.4.4\n",
      "jupyterlab-pygments==0.2.2\n",
      "keras==2.12.0\n",
      "kiwisolver==1.4.4\n",
      "langcodes==3.3.0\n",
      "libclang==16.0.0\n",
      "Markdown==3.4.3\n",
      "MarkupSafe==2.1.2\n",
      "matplotlib==3.6.2\n",
      "matplotlib-inline==0.1.6\n",
      "matplotx==0.3.10\n",
      "mistune==2.0.4\n",
      "ml-dtypes==0.1.0\n",
      "mpmath==1.3.0\n",
      "murmurhash==1.0.9\n",
      "nbclassic==0.4.8\n",
      "nbclient==0.7.2\n",
      "nbconvert==7.2.8\n",
      "nbformat==5.7.3\n",
      "nest-asyncio==1.5.6\n",
      "networkx==3.1\n",
      "nltk==3.7\n",
      "notebook==6.5.2\n",
      "notebook_shim==0.2.2\n",
      "numpy==1.23.5\n",
      "oauthlib==3.2.2\n",
      "openpyxl==3.1.2\n",
      "opt-einsum==3.3.0\n",
      "packaging==22.0\n",
      "pandas==1.5.2\n",
      "pandocfilters==1.5.0\n",
      "params-flow==0.8.2\n",
      "parso==0.8.3\n",
      "pathy==0.10.1\n",
      "pdfminer==20191125\n",
      "pefile==2022.5.30\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.3.0\n",
      "platformdirs==2.6.2\n",
      "preshed==3.0.8\n",
      "prometheus-client==0.15.0\n",
      "prompt-toolkit==3.0.36\n",
      "protobuf==4.22.3\n",
      "psutil==5.9.4\n",
      "pure-eval==0.2.2\n",
      "py-params==0.10.2\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.21\n",
      "pycryptodome==3.16.0\n",
      "pydantic==1.10.4\n",
      "pygame==2.1.2\n",
      "Pygments==2.14.0\n",
      "pyinstaller==5.7.0\n",
      "pyinstaller-hooks-contrib==2022.15\n",
      "pymystem3==0.2.0\n",
      "pyparsing==3.0.9\n",
      "pyrsistent==0.19.3\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.4\n",
      "pytz==2022.6\n",
      "pywin32==305\n",
      "pywin32-ctypes==0.2.0\n",
      "pywinpty==2.0.10\n",
      "PyYAML==6.0\n",
      "pyzmq==25.0.0\n",
      "razdel==0.5.0\n",
      "regex==2022.10.31\n",
      "requests==2.28.1\n",
      "requests-oauthlib==1.3.1\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rsa==4.9\n",
      "scikit-learn==1.2.2\n",
      "scipy==1.10.1\n",
      "seaborn==0.12.2\n",
      "Send2Trash==1.8.0\n",
      "sentence-transformers==2.2.2\n",
      "sentencepiece==0.1.97\n",
      "six==1.16.0\n",
      "smart-open==6.3.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.3.2.post1\n",
      "spacy==3.4.4\n",
      "spacy-legacy==3.0.11\n",
      "spacy-loggers==1.0.4\n",
      "spacy-spanish-lemmatizer==0.7\n",
      "srsly==2.4.5\n",
      "stack-data==0.6.2\n",
      "sympy==1.11.1\n",
      "tensorboard==2.12.1\n",
      "tensorboard-data-server==0.7.0\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorflow==2.12.0\n",
      "tensorflow-estimator==2.12.0\n",
      "tensorflow-hub==0.13.0\n",
      "tensorflow-intel==2.12.0\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "termcolor==2.2.0\n",
      "terminado==0.17.1\n",
      "textblob==0.17.1\n",
      "thinc==8.1.7\n",
      "threadpoolctl==3.1.0\n",
      "tinycss2==1.2.1\n",
      "tokenization==1.0.7\n",
      "tokenizers==0.13.3\n",
      "torch==2.0.0\n",
      "torchvision==0.15.1\n",
      "tornado==6.2\n",
      "tqdm==4.64.1\n",
      "traitlets==5.8.1\n",
      "transformers==4.27.4\n",
      "typer==0.7.0\n",
      "typing_extensions==4.4.0\n",
      "uri-template==1.2.0\n",
      "urllib3==1.26.13\n",
      "wasabi==0.10.1\n",
      "wcwidth==0.2.6\n",
      "webcolors==1.12\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.4.2\n",
      "Werkzeug==2.2.3\n",
      "whichcraft==0.6.1\n",
      "wrapt==1.14.1\n",
      "zipp==3.11.0\n",
      "zope.event==4.6\n",
      "zope.interface==5.5.2\n"
     ]
    }
   ],
   "source": [
    "! pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
